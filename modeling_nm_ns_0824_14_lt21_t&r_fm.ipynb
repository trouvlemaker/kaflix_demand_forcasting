{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0517062-4cb2-485e-8a18-f1f474c9a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and set PROJECT_ID and REGION environment variables.\n",
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "REGION = 'asia-northeast3'\n",
    "# Google Cloud Storage bucket for artifact storage.\n",
    "BUCKET = 'mlops-test-kay'\n",
    "BUCKET_URI = 'gs://' + BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b315d9f8-b085-430b-bc3f-5f316c033c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sklearn import preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbb165ee-ada8-422b-b50e-cf811318816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"tf-cpu.2-6\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-6\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36c6163b-5368-4bfd-950f-0d1a705179cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT, location=REGION, staging_bucket=BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "216a238a-ec93-46eb-a221-3b3c30ccfe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-8\n",
      "Deploy machine type n1-standard-8\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"8\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"8\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ec22335-8d61-4942-89c8-9bd90fcacb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "JOB_NAME = \"airpot_passenger-\" + TIMESTAMP\n",
    "\n",
    "# Training parameters\n",
    "MODEL_NAME = 'airport_passenger'\n",
    "\n",
    "EPOCHS = 400\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT = 0.05\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--learning_rate=\" + str(LEARNING_RATE),\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--dropout=\" + str(DROPOUT)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fe50214-b461-459c-bdce-92b29d773b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded storage object airport_passenger/X_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl from bucket mlops-test-kay to local file X_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl.\n",
      "Downloaded storage object airport_passenger/X_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl from bucket mlops-test-kay to local file X_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl.\n",
      "Downloaded storage object airport_passenger/Y_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl from bucket mlops-test-kay to local file Y_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl.\n",
      "Downloaded storage object airport_passenger/Y_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl from bucket mlops-test-kay to local file Y_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl.\n"
     ]
    }
   ],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation, visualization and useful functions\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "# gcp functions\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Keras and tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Download data\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your GCS object\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # The path to which the file should be downloaded\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "            source_blob_name, bucket_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/X_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\", \"X_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/X_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\", \"X_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/Y_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\", \"Y_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/Y_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\", \"Y_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\")\n",
    "\n",
    "with open('X_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl','rb') as f:\n",
    "    x_val = pickle.load(f)\n",
    "\n",
    "with open('X_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl','rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "\n",
    "with open('Y_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl','rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open('Y_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl','rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "    \n",
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "x_val  = np.asarray(x_val).astype(np.float32)\n",
    "y_val  = np.asarray(y_val).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee20c63e-2da7-4cba-9486-c243922e7e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 7, 420)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c958a4c-ae3d-4690-8ac2-a3a0a8017b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "\n",
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation, visualization and useful functions\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "# gcp functions\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Keras and tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Download data\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your GCS object\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # The path to which the file should be downloaded\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "            source_blob_name, bucket_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/X_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\", \"X_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/X_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\", \"X_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/Y_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\", \"Y_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/Y_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\", \"Y_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl\")\n",
    "\n",
    "with open('X_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl','rb') as f:\n",
    "    x_val = pickle.load(f)\n",
    "\n",
    "with open('X_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl','rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "\n",
    "with open('Y_val_multi_scaled_nm_ns_14_lt21_tr_fm.pkl','rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open('Y_train_multi_scaled_nm_ns_14_lt21_tr_fm.pkl','rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "    \n",
    "    \n",
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "x_val  = np.asarray(x_val).astype(np.float32)\n",
    "y_val  = np.asarray(y_val).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "past = 7\n",
    "n_steps = 7\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--learning_rate', dest='learning_rate',\n",
    "                    default=0.001, type=float,\n",
    "                    help='Learning rate')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=400, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=8, type=int,\n",
    "                    help='Batch size.')\n",
    "parser.add_argument('--dropout', dest='dropout', \n",
    "                    default=0.05, type=float,\n",
    "                    help='Dropput ratio')\n",
    "args = parser.parse_args()\n",
    "\n",
    "def build_model(X_train_multi_gru):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True, kernel_initializer='he_normal',activation='relu'))\n",
    "    model.add(Dropout(args.dropout)) \n",
    "    model.add(GRU(768, return_sequences=True, kernel_initializer='he_normal',activation=\"relu\"))\n",
    "    model.add(Dropout(args.dropout)) \n",
    "    model.add(GRU(512, return_sequences=True, kernel_initializer='he_normal',activation=\"relu\"))\n",
    "    model.add(Dropout(args.dropout)) \n",
    "    model.add(GRU(64, return_sequences=False, kernel_initializer='he_normal',activation=\"relu\"))\n",
    "    model.add(Dropout(args.dropout)) \n",
    "    model.add(Dense(n_steps))\n",
    "    model.compile(keras.optimizers.Adam(learning_rate=args.learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model = build_model(x_train)\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "model.fit(x_train, y_train, \n",
    "                      batch_size=args.batch_size, epochs=args.epochs,validation_data=(x_val, y_val), shuffle=True,\n",
    "                      verbose=1,)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea6a65da-3d75-403b-a893-64fcb2878776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://mlops-test-kay/aiplatform-2022-08-25-08:52:00.432-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://mlops-test-kay/aiplatform-custom-training-2022-08-25-08:52:00.492 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/asia-northeast3/training/4223699151310815232?project=392016637758\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/asia-northeast3/training/826014707436486656?project=392016637758\n",
      "CustomTrainingJob projects/392016637758/locations/asia-northeast3/trainingPipelines/4223699151310815232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/392016637758/locations/asia-northeast3/trainingPipelines/4223699151310815232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/392016637758/locations/asia-northeast3/trainingPipelines/4223699151310815232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/392016637758/locations/asia-northeast3/trainingPipelines/4223699151310815232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/392016637758/locations/asia-northeast3/trainingPipelines/4223699151310815232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/392016637758/locations/asia-northeast3/trainingPipelines/4223699151310815232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/392016637758/locations/asia-northeast3/trainingPipelines/4223699151310815232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob run completed. Resource name: projects/392016637758/locations/asia-northeast3/trainingPipelines/4223699151310815232\n",
      "Model available at projects/392016637758/locations/asia-northeast3/models/2189611436018237440\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    #requirements=[\"google-cloud-bigquery>=2.20.0\", \"db-dtypes\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"airpot_passenger-\" + TIMESTAMP\n",
    "\n",
    "# Start the training\n",
    "model = job.run(\n",
    "    #dataset=dataset,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    #bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c9a00-19f1-4ac2-9638-562fa834fcac",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "002cc62e-0e50-4633-a83c-0f96bdc64ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/392016637758/locations/asia-northeast3/endpoints/3003443554618966016/operations/3398168228990550016\n",
      "Endpoint created. Resource name: projects/392016637758/locations/asia-northeast3/endpoints/3003443554618966016\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/392016637758/locations/asia-northeast3/endpoints/3003443554618966016')\n",
      "Deploying model to Endpoint : projects/392016637758/locations/asia-northeast3/endpoints/3003443554618966016\n",
      "Deploy Endpoint model backing LRO: projects/392016637758/locations/asia-northeast3/endpoints/3003443554618966016/operations/8192250032326443008\n",
      "Endpoint model deployed. Resource name: projects/392016637758/locations/asia-northeast3/endpoints/3003443554618966016\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = f\"{MODEL_NAME}_deployed-\" + TIMESTAMP\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    traffic_split={\"0\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a8ab0-ffe4-4e3a-86c0-39ee8fe7d236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38ce0e-629a-4af8-a2a4-85a6f1e12f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf775a11-adc3-46eb-80a8-aa50c35e2e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9e8b2-1fed-4b6e-bb3e-e270bde51e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a2b57-dbf4-4534-a961-be0780e85e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6b674-b62d-4f3e-a5c0-3c3cbca6177e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
