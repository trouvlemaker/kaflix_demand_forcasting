{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0517062-4cb2-485e-8a18-f1f474c9a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and set PROJECT_ID and REGION environment variables.\n",
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "REGION = 'asia-northeast3'\n",
    "# Google Cloud Storage bucket for artifact storage.\n",
    "BUCKET = 'mlops-test-kay'\n",
    "BUCKET_URI = 'gs://' + BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b315d9f8-b085-430b-bc3f-5f316c033c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sklearn import preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbb165ee-ada8-422b-b50e-cf811318816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"tf-cpu.2-6\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-6\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36c6163b-5368-4bfd-950f-0d1a705179cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT, location=REGION, staging_bucket=BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "216a238a-ec93-46eb-a221-3b3c30ccfe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-8\n",
      "Deploy machine type n1-standard-8\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"8\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"8\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ec22335-8d61-4942-89c8-9bd90fcacb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "JOB_NAME = \"airpot_passenger-\" + TIMESTAMP\n",
    "\n",
    "# Training parameters\n",
    "MODEL_NAME = 'airport_passenger'\n",
    "\n",
    "EPOCHS = 400\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT = 0.05\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--learning_rate=\" + str(LEARNING_RATE),\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--dropout=\" + str(DROPOUT)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fe50214-b461-459c-bdce-92b29d773b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded storage object airport_passenger/X_train_multi_scaled_nm_ns_7_lt21_tr.pkl from bucket mlops-test-kay to local file X_train_multi_scaled_nm_ns_7_lt21_tr.pkl.\n",
      "Downloaded storage object airport_passenger/X_val_multi_scaled_nm_ns_7_lt21_tr.pkl from bucket mlops-test-kay to local file X_val_multi_scaled_nm_ns_7_lt21_tr.pkl.\n",
      "Downloaded storage object airport_passenger/Y_train_multi_scaled_nm_ns_7_lt21_tr.pkl from bucket mlops-test-kay to local file Y_train_multi_scaled_nm_ns_7_lt21_tr.pkl.\n",
      "Downloaded storage object airport_passenger/Y_val_multi_scaled_nm_ns_7_lt21_tr.pkl from bucket mlops-test-kay to local file Y_val_multi_scaled_nm_ns_7_lt21_tr.pkl.\n"
     ]
    }
   ],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation, visualization and useful functions\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "# gcp functions\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Keras and tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Download data\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your GCS object\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # The path to which the file should be downloaded\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "            source_blob_name, bucket_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/X_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\", \"X_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/X_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\", \"X_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/Y_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\", \"Y_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/Y_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\", \"Y_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\")\n",
    "\n",
    "with open('X_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl','rb') as f:\n",
    "    x_val = pickle.load(f)\n",
    "\n",
    "with open('X_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl','rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "\n",
    "with open('Y_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl','rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open('Y_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl','rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "    \n",
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "x_val  = np.asarray(x_val).astype(np.float32)\n",
    "y_val  = np.asarray(y_val).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee20c63e-2da7-4cba-9486-c243922e7e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 7, 420)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c958a4c-ae3d-4690-8ac2-a3a0a8017b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "\n",
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation, visualization and useful functions\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "# gcp functions\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Keras and tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Download data\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your GCS object\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # The path to which the file should be downloaded\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "            source_blob_name, bucket_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/X_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\", \"X_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/X_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\", \"X_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/Y_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\", \"Y_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\")\n",
    "download_blob(\"mlops-test-kay\", \"airport_passenger/Y_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\", \"Y_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl\")\n",
    "\n",
    "with open('X_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl','rb') as f:\n",
    "    x_val = pickle.load(f)\n",
    "\n",
    "with open('X_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl','rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "\n",
    "with open('Y_val_multi_scaled_nm_ns_7_lt21_tr_fm.pkl','rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open('Y_train_multi_scaled_nm_ns_7_lt21_tr_fm.pkl','rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "    \n",
    "    \n",
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "x_val  = np.asarray(x_val).astype(np.float32)\n",
    "y_val  = np.asarray(y_val).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "past = 7\n",
    "n_steps = 7\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--learning_rate', dest='learning_rate',\n",
    "                    default=0.001, type=float,\n",
    "                    help='Learning rate')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=400, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=8, type=int,\n",
    "                    help='Batch size.')\n",
    "parser.add_argument('--dropout', dest='dropout', \n",
    "                    default=0.05, type=float,\n",
    "                    help='Dropput ratio')\n",
    "args = parser.parse_args()\n",
    "\n",
    "def build_model(X_train_multi_gru):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True, kernel_initializer='he_normal',activation='relu'))\n",
    "    model.add(Dropout(args.dropout)) \n",
    "    model.add(GRU(768, return_sequences=True, kernel_initializer='he_normal',activation=\"relu\"))\n",
    "    model.add(Dropout(args.dropout)) \n",
    "    model.add(GRU(512, return_sequences=True, kernel_initializer='he_normal',activation=\"relu\"))\n",
    "    model.add(Dropout(args.dropout)) \n",
    "    model.add(GRU(64, return_sequences=False, kernel_initializer='he_normal',activation=\"relu\"))\n",
    "    model.add(Dropout(args.dropout)) \n",
    "    model.add(Dense(n_steps))\n",
    "    model.compile(keras.optimizers.Adam(learning_rate=args.learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model = build_model(x_train)\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "model.fit(x_train, y_train, \n",
    "                      batch_size=args.batch_size, epochs=args.epochs,validation_data=(x_val, y_val), shuffle=True,\n",
    "                      verbose=1,)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea6a65da-3d75-403b-a893-64fcb2878776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://mlops-test-kay/aiplatform-2022-08-25-06:23:02.198-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://mlops-test-kay/aiplatform-custom-training-2022-08-25-06:23:02.258 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/asia-northeast3/training/4660548315165753344?project=392016637758\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/asia-northeast3/training/4570194847641632768?project=392016637758\n",
      "CustomTrainingJob projects/392016637758/locations/asia-northeast3/trainingPipelines/4660548315165753344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13522/1920563537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCMDARGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mreplica_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     machine_type=TRAIN_COMPUTE)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, tensorboard, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m   3186\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3187\u001b[0m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3188\u001b[0;31m             \u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3189\u001b[0m         )\n\u001b[1;32m   3190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, python_packager, dataset, annotation_schema_uri, worker_pool_specs, managed_model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, args, environment_variables, base_output_dir, service_account, network, bigquery_destination, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, tensorboard, reduction_server_container_uri, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m   3484\u001b[0m             \u001b[0mgcs_destination_uri_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_output_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3485\u001b[0m             \u001b[0mbigquery_destination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigquery_destination\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3486\u001b[0;31m             \u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3487\u001b[0m         )\n\u001b[1;32m   3488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_run_job\u001b[0;34m(self, training_task_definition, training_task_inputs, dataset, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, annotation_schema_uri, model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, gcs_destination_uri_prefix, bigquery_destination, create_request_timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"View Training:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dashboard_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_get_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mTraining\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_failed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0mprevious_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_JOB_WAIT_TIME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    #requirements=[\"google-cloud-bigquery>=2.20.0\", \"db-dtypes\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"airpot_passenger-\" + TIMESTAMP\n",
    "\n",
    "# Start the training\n",
    "model = job.run(\n",
    "    #dataset=dataset,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    #bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c9a00-19f1-4ac2-9638-562fa834fcac",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002cc62e-0e50-4633-a83c-0f96bdc64ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_NAME = f\"{MODEL_NAME}_deployed-\" + TIMESTAMP\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    traffic_split={\"0\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a8ab0-ffe4-4e3a-86c0-39ee8fe7d236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38ce0e-629a-4af8-a2a4-85a6f1e12f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf775a11-adc3-46eb-80a8-aa50c35e2e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9e8b2-1fed-4b6e-bb3e-e270bde51e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a2b57-dbf4-4534-a961-be0780e85e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6b674-b62d-4f3e-a5c0-3c3cbca6177e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
