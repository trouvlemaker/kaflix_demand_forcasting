{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcbbcf7-363c-41aa-8a8d-8c794b2beafd",
   "metadata": {},
   "source": [
    "## 향후 7일 입도객 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c747fd5-56ad-451f-8367-e57167afd18b",
   "metadata": {},
   "source": [
    "#### 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc36259d-3b88-4589-8a95-5f99793d2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation, visualization and useful functions\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from datetime import date, timedelta\n",
    "import missingno as msno\n",
    "\n",
    "# gcp functions\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Keras and tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcbcc2e-1ccd-4b61-8790-8708e49a6aae",
   "metadata": {},
   "source": [
    "#### 데이터 쿼리 from 빅쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f54bda-1a96-4fdd-9363-258eb876f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up BigQuery clients\n",
    "bqclient = bigquery.Client(project='charged-genre-350106')\n",
    "\n",
    "# Query for base dataset\n",
    "\n",
    "query_AP = \"\"\"\n",
    "    SELECT\n",
    "      CONCAT(CAST(FORMAT_DATE(\"%E4Y\", CAST(DT as date)) AS string),\n",
    "             CAST(FORMAT_DATE(\"%m\", CAST(DT as date)) AS string),\n",
    "             CAST(FORMAT_DATE(\"%d\", CAST(DT as date)) AS string)) AS DT\n",
    "      , ARRIVE_PPL as ARRIVE_PPL\n",
    "    FROM `charged-genre-350106.kaflix.AIRLINE_PASSENGER`\n",
    "    \"\"\"\n",
    "\n",
    "query_NS = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `charged-genre-350106.kaflix.NAVER_SEARCH`\n",
    "    \"\"\"\n",
    "\n",
    "query_AT = \"\"\"\n",
    "    WITH MAIN AS (\n",
    "        SELECT *\n",
    "        FROM kaflix.AIRLINE_TICKET A\n",
    "        WHERE 1=1\n",
    "            -- AND A.DEPART = 'GMP'\n",
    "            -- AND A.ARRIVE = 'CJU'\n",
    "        AND A.SEARCH_DATE NOT IN ('2022-07-08', '2022-07-09', '2022-07-10',  '2022-07-12')\n",
    "    )\n",
    "    SELECT\n",
    "        A.*\n",
    "        , B.B\n",
    "        , B.F\n",
    "        , B.D\n",
    "        , B.S\n",
    "        , B.SD\n",
    "    FROM(\n",
    "        SELECT\n",
    "           CONCAT(CAST(FORMAT_DATE(\"%E4Y\", CAST(LEFT(DEPART_DATE,10) as date)) AS string),\n",
    "                  CAST(FORMAT_DATE(\"%m\", CAST(LEFT(DEPART_DATE,10) as date)) AS string),\n",
    "                  CAST(FORMAT_DATE(\"%d\", CAST(LEFT(DEPART_DATE,10) as date)) AS string)) AS DEPART_DT\n",
    "           , CONCAT(CAST(FORMAT_DATE(\"%E4Y\", CAST(LEFT(SEARCH_DATE, 10) as date)) AS string),\n",
    "                    CAST(FORMAT_DATE(\"%m\", CAST(LEFT(SEARCH_DATE,10) as date)) AS string),\n",
    "                    CAST(FORMAT_DATE(\"%d\", CAST(LEFT(SEARCH_DATE,10) as date)) AS string)) AS SEARCH_DT\n",
    "           , CAST(FORMAT_DATE(\"%a\", CAST(LEFT(DEPART_DATE, 10) as date)) AS string) AS DOW\n",
    "           , DATE_DIFF(CAST(LEFT(DEPART_DATE,10) AS date), CAST(LEFT(SEARCH_DATE,10) as date), DAY) AS LEAD_TM\n",
    "        , COUNT(*) AS TICKET\n",
    "        , MIN(FARE) AS FARE_MIN\n",
    "        , MAX(FARE) AS FARE_MAX\n",
    "        , AVG(FARE) AS FARE_AVG\n",
    "        , STDDEV(FARE) AS FARE_STD\n",
    "        , MIN(AVAIL_SEAT) AS SEAT_MIN\n",
    "        , MAX(AVAIL_SEAT) AS SEAT_MAX\n",
    "        , AVG(AVAIL_SEAT) AS SEAT_AVG\n",
    "        , STDDEV(AVAIL_SEAT) AS SEAT_STD\n",
    "        FROM MAIN\n",
    "        WHERE 1=1\n",
    "        GROUP BY \n",
    "        DEPART_DATE, SEARCH_DATE\n",
    "    ) A\n",
    "    LEFT JOIN (\n",
    "        SELECT *\n",
    "        FROM(\n",
    "            SELECT\n",
    "            CONCAT(CAST(FORMAT_DATE(\"%E4Y\", CAST(LEFT(DEPART_DATE,10) as date)) AS string), CAST(FORMAT_DATE(\"%m\", CAST(LEFT(DEPART_DATE,10) as date)) AS string), CAST(FORMAT_DATE(\"%d\", CAST(LEFT(DEPART_DATE,10) as date)) AS string)) AS DEPART_DT\n",
    "            , CONCAT(CAST(FORMAT_DATE(\"%E4Y\", CAST(LEFT(SEARCH_DATE, 10) as date)) AS string), CAST(FORMAT_DATE(\"%m\", CAST(LEFT(SEARCH_DATE,10) as date)) AS string), CAST(FORMAT_DATE(\"%d\", CAST(LEFT(SEARCH_DATE,10) as date)) AS string)) AS SEARCH_DT \n",
    "                , CLASS_DESC\n",
    "            FROM MAIN\n",
    "            WHERE 1=1\n",
    "        ) A\n",
    "        PIVOT (\n",
    "            COUNT(*) \n",
    "            FOR CLASS_DESC IN ('비즈니스석' AS B, '일반석' AS F, '할인석' AS D, '특가석' AS S, '단독특가' AS SD)\n",
    "        )\n",
    "    ) B ON A.DEPART_DT = B.DEPART_DT AND A.SEARCH_DT = B.SEARCH_DT\n",
    "    WHERE 1=1\n",
    "        -- AND A.DEPART_DT < '20220725'\n",
    "    ORDER BY \n",
    "      A.DEPART_DT DESC\n",
    "      , A.LEAD_TM\n",
    "    \"\"\"\n",
    "\n",
    "query_HD = \"\"\"\n",
    "    SELECT\n",
    "      CONCAT(CAST(FORMAT_DATE(\"%E4Y\", CAST(DT as date)) AS string),\n",
    "             CAST(FORMAT_DATE(\"%m\", CAST(DT as date)) AS string),\n",
    "             CAST(FORMAT_DATE(\"%d\", CAST(DT as date)) AS string)) AS DT\n",
    "      , TOURIST AS TOURIST\n",
    "      , TEMPERTURE AS TEMPERTURE\n",
    "      , RAIN AS RAIN \n",
    "      , HOLIDAY_NAME AS HOLIDAY_NAME\n",
    "    FROM `charged-genre-350106.kaflix.TOURIST_WEATHER`\n",
    "    \"\"\"\n",
    "\n",
    "query_RC = \"\"\"\n",
    "    SELECT\n",
    "      PARSE_DATE('%Y%m%d', PURCHASE_DATE) AS PURCHASE_DT\n",
    "      , PARSE_DATE('%Y%m%d', BEGIN_DATE) AS BEGIN_DT\n",
    "      , DATE_DIFF (PARSE_DATE('%Y%m%d', BEGIN_DATE),PARSE_DATE('%Y%m%d', PURCHASE_DATE), DAY) AS LEAD_TM\n",
    "    FROM `charged-genre-350106.kaflix.ERP_MERGING`\n",
    "    \"\"\"\n",
    "\n",
    "query_TW = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `charged-genre-350106.kaflix.TWAY_KAFLIX`\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83558233-b166-4b57-918d-9757b6be6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = bqclient.query(query_TW).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb7dd30-a2f1-4b7c-a7b8-80ea6710c221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEARCH_DATE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>DEPART_DATE</th>\n",
       "      <th>DEPART</th>\n",
       "      <th>DEPART_TIME</th>\n",
       "      <th>ARRIVE_TIME</th>\n",
       "      <th>FLIGHT_MODEL</th>\n",
       "      <th>TOTAL_SEAT</th>\n",
       "      <th>GROUP_SOLD</th>\n",
       "      <th>TOTAL_SOLD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-11</td>\n",
       "      <td>TW505</td>\n",
       "      <td>2022-03-11</td>\n",
       "      <td>GMP</td>\n",
       "      <td>2022-03-11 08:15:00</td>\n",
       "      <td>2022-03-11 09:30:00</td>\n",
       "      <td>330</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-11</td>\n",
       "      <td>TW507</td>\n",
       "      <td>2022-03-11</td>\n",
       "      <td>GMP</td>\n",
       "      <td>2022-03-11 13:40:00</td>\n",
       "      <td>2022-03-11 14:55:00</td>\n",
       "      <td>330</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-14</td>\n",
       "      <td>TW509</td>\n",
       "      <td>2022-03-14</td>\n",
       "      <td>GMP</td>\n",
       "      <td>2022-03-14 10:00:00</td>\n",
       "      <td>2022-03-14 11:15:00</td>\n",
       "      <td>330</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-15</td>\n",
       "      <td>TW517</td>\n",
       "      <td>2022-03-15</td>\n",
       "      <td>GMP</td>\n",
       "      <td>2022-03-15 11:00:00</td>\n",
       "      <td>2022-03-15 12:20:00</td>\n",
       "      <td>330</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>TW701</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>GMP</td>\n",
       "      <td>2022-01-01 06:40:00</td>\n",
       "      <td>2022-01-01 07:50:00</td>\n",
       "      <td>737</td>\n",
       "      <td>189</td>\n",
       "      <td>7</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424785</th>\n",
       "      <td>2022-03-22</td>\n",
       "      <td>TW9945</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>PUS</td>\n",
       "      <td>2022-03-26 09:55:00</td>\n",
       "      <td>2022-03-26 10:55:00</td>\n",
       "      <td>737</td>\n",
       "      <td>189</td>\n",
       "      <td>7</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424786</th>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>TW9945</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>PUS</td>\n",
       "      <td>2022-03-26 09:55:00</td>\n",
       "      <td>2022-03-26 10:55:00</td>\n",
       "      <td>737</td>\n",
       "      <td>189</td>\n",
       "      <td>7</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424787</th>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>TW9945</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>PUS</td>\n",
       "      <td>2022-03-26 09:55:00</td>\n",
       "      <td>2022-03-26 10:55:00</td>\n",
       "      <td>737</td>\n",
       "      <td>189</td>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424788</th>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>TW9945</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>PUS</td>\n",
       "      <td>2022-03-26 09:55:00</td>\n",
       "      <td>2022-03-26 10:55:00</td>\n",
       "      <td>737</td>\n",
       "      <td>189</td>\n",
       "      <td>5</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424789</th>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>TW9945</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>PUS</td>\n",
       "      <td>2022-03-26 09:55:00</td>\n",
       "      <td>2022-03-26 10:55:00</td>\n",
       "      <td>737</td>\n",
       "      <td>189</td>\n",
       "      <td>5</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424790 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SEARCH_DATE FLIGHT_NUMBER DEPART_DATE DEPART          DEPART_TIME  \\\n",
       "0       2022-03-11         TW505  2022-03-11    GMP  2022-03-11 08:15:00   \n",
       "1       2022-03-11         TW507  2022-03-11    GMP  2022-03-11 13:40:00   \n",
       "2       2022-03-14         TW509  2022-03-14    GMP  2022-03-14 10:00:00   \n",
       "3       2022-03-15         TW517  2022-03-15    GMP  2022-03-15 11:00:00   \n",
       "4       2022-01-01         TW701  2022-01-01    GMP  2022-01-01 06:40:00   \n",
       "...            ...           ...         ...    ...                  ...   \n",
       "424785  2022-03-22        TW9945  2022-03-26    PUS  2022-03-26 09:55:00   \n",
       "424786  2022-03-23        TW9945  2022-03-26    PUS  2022-03-26 09:55:00   \n",
       "424787  2022-03-24        TW9945  2022-03-26    PUS  2022-03-26 09:55:00   \n",
       "424788  2022-03-25        TW9945  2022-03-26    PUS  2022-03-26 09:55:00   \n",
       "424789  2022-03-26        TW9945  2022-03-26    PUS  2022-03-26 09:55:00   \n",
       "\n",
       "                ARRIVE_TIME  FLIGHT_MODEL  TOTAL_SEAT  GROUP_SOLD  TOTAL_SOLD  \n",
       "0       2022-03-11 09:30:00           330           1           0           1  \n",
       "1       2022-03-11 14:55:00           330          10           0           2  \n",
       "2       2022-03-14 11:15:00           330           8           0           2  \n",
       "3       2022-03-15 12:20:00           330           6           0           3  \n",
       "4       2022-01-01 07:50:00           737         189           7         169  \n",
       "...                     ...           ...         ...         ...         ...  \n",
       "424785  2022-03-26 10:55:00           737         189           7         109  \n",
       "424786  2022-03-26 10:55:00           737         189           7         113  \n",
       "424787  2022-03-26 10:55:00           737         189           5         116  \n",
       "424788  2022-03-26 10:55:00           737         189           5         189  \n",
       "424789  2022-03-26 10:55:00           737         189           5         188  \n",
       "\n",
       "[424790 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058d8825-e9f0-4d33-b3da-ecbcd8c0b71d",
   "metadata": {},
   "source": [
    "#### 데이터 로드 from 빅쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68ed98b-db53-444f-90d5-da5c96c0b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Load\n",
    "def data_load(AP, AT, HD, RC, TW, start_date, end_date):\n",
    "    psg = bqclient.query(AP).to_dataframe()\n",
    "    air = bqclient.query(AT).to_dataframe()\n",
    "    wth = bqclient.query(HD).to_dataframe()\n",
    "    rc = bqclient.query(RC).to_dataframe()\n",
    "    tw = bqclient.query(TW).to_dataframe()\n",
    "    \n",
    "    # converting datetime\n",
    "    psg['DT']=pd.to_datetime(psg['DT'])\n",
    "    air['DEPART_DT']=pd.to_datetime(air['DEPART_DT'])\n",
    "    wth['DT']=pd.to_datetime(wth['DT'])\n",
    "    rc['BEGIN_DT']=pd.to_datetime(rc['BEGIN_DT'])\n",
    "    tw['SEARCH_DATE']=pd.to_datetime(tw['SEARCH_DATE'])\n",
    "    tw['DEPART_DATE']=pd.to_datetime(tw['DEPART_DATE'])\n",
    "\n",
    "    # air ticket 7days lagged to predict next 7days\n",
    "    air = air.query(\"8<=LEAD_TM <= 28\")\n",
    "    rc=rc.query('8<=LEAD_TM<=28')\n",
    "\n",
    "    # Indexing pgs date & left join with other data\n",
    "    psg.set_index(\"DT\", inplace=True)\n",
    "    psg = psg.loc[(psg.index >= start_date) & (psg.index<= end_date),:]  # 0513까지 삭제\n",
    "    psg = psg.groupby(\"DT\")['ARRIVE_PPL'].sum() # 일별 입도객 합계\n",
    "    psg = pd.DataFrame(psg)                     # 데이터프레임만들고\n",
    "    \n",
    "    # column rename to DT\n",
    "    air.rename(columns={'DEPART_DT':'DT'}, inplace=True)\n",
    "    rc.rename(columns={'BEGIN_DT':'DT'}, inplace=True)\n",
    "    #tw.rename(columns={'DEPART_DATE':'DT'}, inplace=True)\n",
    "\n",
    "    wth = wth[['DT','TEMPERTURE','RAIN','HOLIDAY_NAME']]\n",
    "    air = air[['DT','LEAD_TM','TICKET','FARE_MIN','FARE_MAX','FARE_AVG', 'FARE_STD', 'SEAT_MIN', 'SEAT_MAX', 'SEAT_AVG', 'SEAT_STD','B','F','D','S','SD']]\n",
    "    return psg, air, wth, rc, tw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee840b-141a-406d-bd92-af46952f9b4e",
   "metadata": {},
   "source": [
    "### 렌타카 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d79297-9e1e-4f4a-9ddc-27c7106e7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rentacar_processing(df):\n",
    "    df=df.groupby(['PURCHASE_DT','DT','LEAD_TM']).size().reset_index()\n",
    "    df.columns=['purchase_dt','DT', 'lead_time','count']\n",
    "    df=df.pivot_table(index=['DT'], columns='lead_time', values='count')\n",
    "    df=df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44a4f9-6e2d-4ae3-8708-e25b7146d3b1",
   "metadata": {},
   "source": [
    "### 티웨이 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ced7f9e-bedd-4ec4-b1eb-555ae98ab447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tway_processing(df):\n",
    "    df1=df[['SEARCH_DATE','DEPART_DATE','FLIGHT_MODEL']]\n",
    "    df1=df1.groupby(['SEARCH_DATE','DEPART_DATE','FLIGHT_MODEL']).size().reset_index()\n",
    "    df1=df1.pivot_table(index=['SEARCH_DATE','DEPART_DATE'], columns='FLIGHT_MODEL', aggfunc=['sum']).reset_index()\n",
    "    df1['lead_time']=df1['DEPART_DATE']-df1['SEARCH_DATE']\n",
    "    df1['lead_time']=df1['lead_time'].dt.days\n",
    "    df1.columns=df1.columns.to_flat_index()\n",
    "    df1.columns=['search_date','depart_date','330','737','lead_time']\n",
    "    df1=df1.fillna(0)\n",
    "\n",
    "    \n",
    "    df2=df[['SEARCH_DATE','DEPART_DATE','GROUP_SOLD','TOTAL_SOLD']]\n",
    "    df2=df2.groupby(['SEARCH_DATE','DEPART_DATE']).sum().reset_index()\n",
    "    df2['lead_time']=df2['DEPART_DATE'] - df2['SEARCH_DATE']\n",
    "    df2['lead_time']=df2['lead_time'].dt.days\n",
    "    df2.columns=['search_date','depart_date','group_sold','total_sold','lead_time']\n",
    "    \n",
    "    df3=pd.merge(df1, df2, how='left', on=['search_date','depart_date'])\n",
    "    df3=df3.drop('lead_time_y', axis=1)\n",
    "    df3.columns=['search_date','depart_date','group_sold','total_sold','lead_time','330','737']\n",
    "    df3=df3.query('8<=lead_time<=28')\n",
    "    \n",
    "    df3=df3.pivot_table(index=['depart_date'], columns='lead_time', values=['group_sold','total_sold','330','737'])\n",
    "    df3.columns=df3.columns.to_flat_index()\n",
    "    df3=df3.reset_index()\n",
    "    df3.columns=['DT','330_8','330_9','330_10','330_11','330_12','330_13','330_14',\n",
    "                      '330_15','330_16','330_17','330_18','330_19','330_20','330_21',\n",
    "                      '330_22','330_23','330_24','330_25','330_26','330_27','330_28',\n",
    "                      '737_8','737_9','737_10','737_11','737_12','737_13','737_14',\n",
    "                      '737_15','737_16','737_17','737_18','737_19','737_20','737_21',\n",
    "                      '737_22','737_23','737_24','737_25','737_26','737_27','737_28',\n",
    "                      'group_sold_8','group_sold_9','group_sold_10','group_sold_11','group_sold_12','group_sold_13','group_sold_14',\n",
    "                      'group_sold_15','group_sold_16','group_sold_17','group_sold_18','group_sold_19','group_sold_20','group_sold_21',\n",
    "                      'group_sold_22','group_sold_23','group_sold_24','group_sold_25','group_sold_26','group_sold_27','group_sold_28',\n",
    "                      'total_sold_8','total_sold_9','total_sold_10','total_sold_11','total_sold_12','total_sold_13','total_sold_14',\n",
    "                      'total_sold_15','total_sold_16','total_sold_17','total_sold_18','total_sold_19','total_sold_20','total_sold_21',\n",
    "                      'total_sold_22','total_sold_23','total_sold_24','total_sold_25','total_sold_26','total_sold_27','total_sold_28']\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab93a15-761e-4747-ab0d-09a98a29536b",
   "metadata": {},
   "source": [
    "#### 휴일 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b9208bb-0544-495b-bb02-1e4301556d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Holiday Data Preprocessing\n",
    "def holiday_data_pre(df):\n",
    "    wth_h = df[['DT', 'HOLIDAY_NAME']]\n",
    "\n",
    "    # 1 for all holidays , 0 for others\n",
    "    wth_h.HOLIDAY_NAME.loc[~wth_h.HOLIDAY_NAME.isnull()] = 1\n",
    "    wth_h = wth_h.sort_values('DT')\n",
    "    wth_h.HOLIDAY_NAME.loc[wth_h.HOLIDAY_NAME.isnull()] = 0\n",
    "    wth_h.HOLIDAY_NAME.unique()\n",
    "\n",
    "    # [long holidays] and [Holidays Interspersed with Workdays]\n",
    "    wth_h['dayofweek'] = wth_h.DT.dt.dayofweek\n",
    "\n",
    "    # 공휴일인데 금요일(4)이면 그주 금요일(4)을 연휴시작으로\n",
    "    # 공휴일인데 월요일(0)이면 전주 토요일(5)을 연휴시작으로\n",
    "    wth_h['flong']=np.where((wth_h['HOLIDAY_NAME'] == 1) & (wth_h['dayofweek'] == 4), 1, 0)\n",
    "    wth_h['mlong']=np.where((wth_h['HOLIDAY_NAME'] == 1) & (wth_h['dayofweek'] == 0), 1, 0)\n",
    "\n",
    "    #월요일 연휴는 그 전 토요일에 연휴 시작 표시\n",
    "    wth_h['mlong']= wth_h['mlong'].shift(-2)\n",
    "    wth_h['mlong'].fillna(method='ffill', inplace = True)\n",
    "\n",
    "    #금요일 연휴, 월요일 연휴 컬럼 합치기\n",
    "    wth_h['long_h']=wth_h['flong'] + wth_h['mlong'] \n",
    "\n",
    "    #연휴시작 하루 전날 표시\n",
    "    wth_h['b_long_h']=wth_h['long_h'].shift(-1)\n",
    "    wth_h['b_long_h'].fillna(method='ffill', inplace = True)\n",
    "\n",
    "    # 징검다리 로직\n",
    "    # 휴일인데 목요일(3)이면 그주 목요일(3)을 징검다리 연휴시작\n",
    "    # 휴일인데 화요일(1)이면 전주 토요일(5)을 징검다리 연휴시작\n",
    "    wth_h['thinter'] = np.where((wth_h['HOLIDAY_NAME'] == 1) & (wth_h['dayofweek'] == 3), 1, 0)\n",
    "    wth_h['tuinter'] = np.where((wth_h['HOLIDAY_NAME'] == 1) & (wth_h['dayofweek'] == 1), 1, 0)\n",
    "\n",
    "    # 화요일 징검다리는 전주 토요일에 연휴 시작 표시\n",
    "    wth_h['tuinter'] = wth_h['tuinter'].shift(-3)\n",
    "    wth_h['tuinter'].fillna(method='ffill', inplace = True)\n",
    "    wth_h['inter_h'] = wth_h['thinter'] + wth_h['tuinter'] \n",
    "\n",
    "    # 징검다리 시작 하루 전날 표시\n",
    "    wth_h['b_inter_h'] = wth_h['inter_h'].shift(-1)\n",
    "    wth_h['b_inter_h'].fillna(method='ffill', inplace = True)\n",
    "\n",
    "    # 명절 연휴 시작일 및 시작일 전날 표시 \n",
    "    #holiday    : 설날/추석 명절 -> 1\n",
    "    #first_m    : 설날/추석 명절연휴 시작일,단 명절 연휴시작이 (일) 또는 (월)이면 (토)가 시작일 -> 1 표시\n",
    "    #b_first_m  : 설날/추석 명절연휴 시작일 전날, 단 명절 연휴시작이 (일) 또는 (월)이면 (금)이 시작일 전날 -> 1로 표시\n",
    "\n",
    "    #first_m    : 설날/추석 명절연휴 시작일,단 명절 연휴시작이 (일) 또는 (월)이면 (토)가 시작일 -> 1 표시\n",
    "    #2017-01-27, 2017-10-03, 2018-02-15, 2018-09-23, 2019-02-04, 2019-09-12, 2020-01-24, 2020-09-30, 2021-02-11, 2021-09-30, 2022-02-01\n",
    "\n",
    "    #first_m_nosm   : 설날/추석 명절연휴 시작일,단 명절 연휴시작이 (화) 또는 (수) 또는 (목) 또는 (금) 또는 (토) 이면 1\n",
    "    wth_h['first_m_nosm']=np.where(((wth_h['DT'] == '2017-01-27') |\n",
    "                              (wth_h['DT'] == '2017-10-03') |\n",
    "                              (wth_h['DT'] == '2018-02-15') |\n",
    "                              (wth_h['DT'] == '2018-09-23') |\n",
    "                              (wth_h['DT'] == '2019-02-04') |\n",
    "                              (wth_h['DT'] == '2019-09-12') |\n",
    "                              (wth_h['DT'] == '2020-01-24') |\n",
    "                              (wth_h['DT'] == '2020-09-30') |\n",
    "                              (wth_h['DT'] == '2021-02-11') |\n",
    "                              (wth_h['DT'] == '2021-09-20') |\n",
    "                              (wth_h['DT'] == '2022-01-31') |\n",
    "                              (wth_h['DT'] == '2022-09-09') )\n",
    "                              & ((wth_h['dayofweek'] != 6) & (wth_h['dayofweek'] !=0)), 1,0)\n",
    "\n",
    "    #first_m_s   : 설날/추석 명절연휴 시작일,단 명절 연휴시작이 (일)이면 1로 표시하고 shift -1\n",
    "    wth_h['first_m_s']=np.where(((wth_h['DT'] == '2017-01-27') |\n",
    "                              (wth_h['DT'] == '2017-10-03') |\n",
    "                              (wth_h['DT'] == '2018-02-15') |\n",
    "                              (wth_h['DT'] == '2018-09-23') |\n",
    "                              (wth_h['DT'] == '2019-02-04') |\n",
    "                              (wth_h['DT'] == '2019-09-12') |\n",
    "                              (wth_h['DT'] == '2020-01-24') |\n",
    "                              (wth_h['DT'] == '2020-09-30') |\n",
    "                              (wth_h['DT'] == '2021-02-11') |\n",
    "                              (wth_h['DT'] == '2021-09-20') |\n",
    "                              (wth_h['DT'] == '2022-01-31') |\n",
    "                              (wth_h['DT'] == '2022-09-09') )\n",
    "                              & (wth_h['dayofweek'] == 6), 1,0)\n",
    "    wth_h['first_m_s'] = wth_h['first_m_s'].shift(-1)\n",
    "    wth_h['first_m_s'].fillna(method='ffill', inplace = True)\n",
    "\n",
    "    #first_m_m   : 설날/추석 명절연휴 시작일,단 명절 연휴시작이 (월)이면 1로 표시하고 shift -2\n",
    "    wth_h['first_m_m']=np.where(((wth_h['DT'] == '2017-01-27') |\n",
    "                              (wth_h['DT'] == '2017-10-03') |\n",
    "                              (wth_h['DT'] == '2018-02-15') |\n",
    "                              (wth_h['DT'] == '2018-09-23') |\n",
    "                              (wth_h['DT'] == '2019-02-04') |\n",
    "                              (wth_h['DT'] == '2019-09-12') |\n",
    "                              (wth_h['DT'] == '2020-01-24') |\n",
    "                              (wth_h['DT'] == '2020-09-30') |\n",
    "                              (wth_h['DT'] == '2021-02-11') |\n",
    "                              (wth_h['DT'] == '2021-09-20') |\n",
    "                              (wth_h['DT'] == '2022-01-31') |\n",
    "                              (wth_h['DT'] == '2022-09-09') )\n",
    "                              & (wth_h['dayofweek'] == 0), 1,0)\n",
    "    wth_h['first_m_m'] = wth_h['first_m_m'].shift(-2)\n",
    "    wth_h['first_m_m'].fillna(method='ffill', inplace = True)\n",
    "\n",
    "    wth_h['first_m'] = wth_h['first_m_nosm'] + wth_h['first_m_s'] + wth_h['first_m_m']\n",
    "\n",
    "    wth_h['b_first_m'] = wth_h['first_m'].shift(-1)\n",
    "    wth_h['b_first_m'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "    wth_h.drop(['flong', 'mlong', 'thinter','tuinter', 'first_m_nosm','first_m_s', 'first_m_m'], axis=1, inplace=True)\n",
    "    return wth_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8bb65a-4184-4f0a-ac51-b40228bab71c",
   "metadata": {},
   "source": [
    "#### 항공티켓 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0293372-e0a0-4e60-93c1-717e56230ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOUIE'S CELL\n",
    "def air_dataprocessing(air):\n",
    "    rename = ['DT', 'ticket_8', 'ticket_9', 'ticket_10','ticket_11','ticket_12','ticket_13','ticket_14',\n",
    "                    'ticket_15','ticket_16', 'ticket_17','ticket_18','ticket_19','ticket_20','ticket_21',\n",
    "                    'ticket_22', 'ticket_23', 'ticket_24','ticket_25','ticket_26','ticket_27','ticket_28',\n",
    "                    'fare_min_8','fare_min_9','fare_min_10','fare_min_11','fare_min_12','fare_min_13','fare_min_14',\n",
    "                    'fare_min_15','fare_min_16','fare_min_17','fare_min_18','fare_min_19','fare_min_20','fare_min_21',\n",
    "                    'fare_min_22','fare_min_23','fare_min_24','fare_min_25','fare_min_26','fare_min_27','fare_min_28',\n",
    "                    'fare_max_8','fare_max_9','fare_max_10','fare_max_11','fare_max_12','fare_max_13','fare_max_14',\n",
    "                    'fare_max_15','fare_max_16','fare_max_17','fare_max_18','fare_max_19','fare_max_20','fare_max_21',\n",
    "                    'fare_max_22','fare_max_23','fare_max_24','fare_max_25','fare_max_26','fare_max_27','fare_max_28',\n",
    "                    'fare_avg_8','fare_avg_9','fare_avg_10','fare_avg_11','fare_avg_12','fare_avg_13','fare_avg_14',\n",
    "                    'fare_avg_15','fare_avg_16','fare_avg_17','fare_avg_18','fare_avg_19','fare_avg_20','fare_avg_21',\n",
    "                    'fare_avg_22','fare_avg_23','fare_avg_24','fare_avg_25','fare_avg_26','fare_avg_27','fare_avg_28',\n",
    "                    'fare_std_8','fare_std_9','fare_std_10','fare_std_11','fare_std_12','fare_std_13','fare_std_14',\n",
    "                    'fare_std_15','fare_std_16','fare_std_17','fare_std_18','fare_std_19','fare_std_20','fare_std_21',\n",
    "                    'fare_std_22','fare_std_23','fare_std_24','fare_std_25','fare_std_26','fare_std_27','fare_std_28',\n",
    "                    'seat_min_8','seat_min_9','seat_min_10','seat_min_11','seat_min_12','seat_min_13','seat_min_14',\n",
    "                    'seat_min_15','seat_min_16','seat_min_17','seat_min_18','seat_min_19','seat_min_20','seat_min_21',\n",
    "                    'seat_min_22','seat_min_23','seat_min_24','seat_min_25','seat_min_26','seat_min_27','seat_min_28',\n",
    "                    'seat_max_8','seat_max_9','seat_max_10','seat_max_11','seat_max_12','seat_max_13','seat_max_14',\n",
    "                    'seat_max_15','seat_max_16','seat_max_17','seat_max_18','seat_max_19','seat_max_20','seat_max_21',\n",
    "                    'seat_max_22','seat_max_23','seat_max_24','seat_max_25','seat_max_26','seat_max_27','seat_max_28',\n",
    "                    'seat_avg_8','seat_avg_9','seat_avg_10','seat_avg_11','seat_avg_12','seat_avg_13','seat_avg_14',\n",
    "                    'seat_avg_15','seat_avg_16','seat_avg_17','seat_avg_18','seat_avg_19','seat_avg_20','seat_avg_21',\n",
    "                    'seat_avg_22','seat_avg_23','seat_avg_24','seat_avg_25','seat_avg_26','seat_avg_27','seat_avg_28',\n",
    "                    'seat_std_8','seat_std_9','seat_std_10','seat_std_11','seat_std_12','seat_std_13','seat_std_14',\n",
    "                    'seat_std_15','seat_std_16','seat_std_17','seat_std_18','seat_std_19','seat_std_20','seat_std_21',\n",
    "                    'seat_std_22','seat_std_23','seat_std_24','seat_std_25','seat_std_26','seat_std_27','seat_std_28',\n",
    "                    'b_8','b_9','b_10','b_11','b_12','b_13','b_14','b_15','b_16','b_17','b_18','b_19','b_20','b_21','b_22','b_23','b_24','b_25','b_26','b_27','b_28',\n",
    "                    'f_8','f_9','f_10','f_11','f_12','f_13','f_14','f_15','f_16','f_17','f_18','f_19','f_20','f_21','f_22','f_23','f_24','f_25','f_26','f_27','f_28',\n",
    "                    'd_8','d_9','d_10','d_11','d_12','d_13','d_14','d_15','d_16','d_17','d_18','d_19','d_20','d_21','d_22','d_23','d_24','d_25','d_26','d_27','d_28',\n",
    "                    's_8','s_9','s_10','s_11','s_12','s_13','s_14','s_15','s_16','s_17','s_18','s_19','s_20','s_21','s_22','s_23','s_24','s_25','s_26','s_27','s_28',\n",
    "                    'sd_8','sd_9','sd_10','sd_11','sd_12','sd_13','sd_14','sd_15','sd_16','sd_17','sd_18','sd_19','sd_20','sd_21','sd_22','sd_23','sd_24','sd_25','sd_26','sd_27','sd_28']\n",
    "\n",
    "    air.columns = rename\n",
    "    \n",
    "                    \n",
    "    datetime_df = air[['DT']]\n",
    "    ticket_df = air[['ticket_8', 'ticket_9', 'ticket_10','ticket_11','ticket_12','ticket_13','ticket_14',\n",
    "                    'ticket_15','ticket_16', 'ticket_17','ticket_18','ticket_19','ticket_20','ticket_21',\n",
    "                    'ticket_22', 'ticket_23', 'ticket_24','ticket_25','ticket_26','ticket_27','ticket_28']]\n",
    "    faremin_df = air[['fare_min_8','fare_min_9','fare_min_10','fare_min_11','fare_min_12','fare_min_13','fare_min_14',\n",
    "                    'fare_min_15','fare_min_16','fare_min_17','fare_min_18','fare_min_19','fare_min_20','fare_min_21',\n",
    "                    'fare_min_22','fare_min_23','fare_min_24','fare_min_25','fare_min_26','fare_min_27','fare_min_28']]\n",
    "    faremax_df = air[['fare_max_8','fare_max_9','fare_max_10','fare_max_11','fare_max_12','fare_max_13','fare_max_14',\n",
    "                    'fare_max_15','fare_max_16','fare_max_17','fare_max_18','fare_max_19','fare_max_20','fare_max_21',\n",
    "                    'fare_max_22','fare_max_23','fare_max_24','fare_max_25','fare_max_26','fare_max_27','fare_max_28']]\n",
    "    fareavg_df = air[['fare_avg_8','fare_avg_9','fare_avg_10','fare_avg_11','fare_avg_12','fare_avg_13','fare_avg_14',\n",
    "                    'fare_avg_15','fare_avg_16','fare_avg_17','fare_avg_18','fare_avg_19','fare_avg_20','fare_avg_21',\n",
    "                    'fare_avg_22','fare_avg_23','fare_avg_24','fare_avg_25','fare_avg_26','fare_avg_27','fare_avg_28']]\n",
    "    farestd_df = air[['fare_std_8','fare_std_9','fare_std_10','fare_std_11','fare_std_12','fare_std_13','fare_std_14',\n",
    "                    'fare_std_15','fare_std_16','fare_std_17','fare_std_18','fare_std_19','fare_std_20','fare_std_21',\n",
    "                    'fare_std_22','fare_std_23','fare_std_24','fare_std_25','fare_std_26','fare_std_27','fare_std_28']]\n",
    "    seatmin_df = air[['seat_min_8','seat_min_9','seat_min_10','seat_min_11','seat_min_12','seat_min_13','seat_min_14',\n",
    "                    'seat_min_15','seat_min_16','seat_min_17','seat_min_18','seat_min_19','seat_min_20','seat_min_21',\n",
    "                    'seat_min_22','seat_min_23','seat_min_24','seat_min_25','seat_min_26','seat_min_27','seat_min_28']]\n",
    "    seatmax_df = air[['seat_max_8','seat_max_9','seat_max_10','seat_max_11','seat_max_12','seat_max_13','seat_max_14',\n",
    "                    'seat_max_15','seat_max_16','seat_max_17','seat_max_18','seat_max_19','seat_max_20','seat_max_21',\n",
    "                    'seat_max_22','seat_max_23','seat_max_24','seat_max_25','seat_max_26','seat_max_27','seat_max_28']]\n",
    "    seatavg_df = air[['seat_avg_8','seat_avg_9','seat_avg_10','seat_avg_11','seat_avg_12','seat_avg_13','seat_avg_14',\n",
    "                    'seat_avg_15','seat_avg_16','seat_avg_17','seat_avg_18','seat_avg_19','seat_avg_20','seat_avg_21',\n",
    "                    'seat_avg_22','seat_avg_23','seat_avg_24','seat_avg_25','seat_avg_26','seat_avg_27','seat_avg_28']]\n",
    "    seatstd_df = air[['seat_std_8','seat_std_9','seat_std_10','seat_std_11','seat_std_12','seat_std_13','seat_std_14',\n",
    "                    'seat_std_15','seat_std_16','seat_std_17','seat_std_18','seat_std_19','seat_std_20','seat_std_21',\n",
    "                    'seat_std_22','seat_std_23','seat_std_24','seat_std_25','seat_std_26','seat_std_27','seat_std_28']]\n",
    "    business_df = air[['b_8','b_9','b_10','b_11','b_12','b_13','b_14','b_15','b_16','b_17','b_18','b_19','b_20','b_21','b_22','b_23','b_24','b_25','b_26','b_27','b_28']]\n",
    "    first_df =    air[['f_8','f_9','f_10','f_11','f_12','f_13','f_14','f_15','f_16','f_17','f_18','f_19','f_20','f_21','f_22','f_23','f_24','f_25','f_26','f_27','f_28']]\n",
    "    discount_df = air[['d_8','d_9','d_10','d_11','d_12','d_13','d_14','d_15','d_16','d_17','d_18','d_19','d_20','d_21','d_22','d_23','d_24','d_25','d_26','d_27','d_28']]\n",
    "    special_df =  air[['s_8','s_9','s_10','s_11','s_12','s_13','s_14','s_15','s_16','s_17','s_18','s_19','s_20','s_21','s_22','s_23','s_24','s_25','s_26','s_27','s_28']]\n",
    "    speciald_df = air[['sd_8','sd_9','sd_10','sd_11','sd_12','sd_13','sd_14','sd_15','sd_16','sd_17','sd_18','sd_19','sd_20','sd_21','sd_22','sd_23','sd_24','sd_25','sd_26','sd_27','sd_28']]\n",
    "\n",
    "\n",
    "    ticket_df = ticket_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    faremin_df = faremin_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    faremax_df = faremax_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    fareavg_df = fareavg_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    farestd_df = farestd_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    seatmin_df = seatmin_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    seatmax_df = seatmax_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    seatavg_df = seatavg_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    seatstd_df = seatstd_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    business_df = business_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    fitst_df = first_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    discount_df = discount_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    special_df = special_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    speciald_df = speciald_df.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "\n",
    "    air2 = pd.concat([datetime_df,ticket_df, faremin_df, faremax_df, fareavg_df, farestd_df, seatmin_df, seatmax_df, seatavg_df, seatstd_df, business_df, fitst_df, discount_df, special_df, speciald_df], axis = 1)\n",
    "    \n",
    "    return air2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33670ea0-d5f8-4854-a016-c3445d8dcc3c",
   "metadata": {},
   "source": [
    "#### 데이터 통합 및 피쳐 엔지니어링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90197c2-b9ca-4cbe-900f-21111243d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Merge\n",
    "def data_merge(psg_df, wth_h_df, air_df, rc_df, tw_df, start_date):\n",
    "    merged = pd.merge(psg_df, wth_h_df, on=\"DT\", how=\"left\")\n",
    "    merged = pd.merge(merged, air_df, on='DT', how='left')\n",
    "    merged = pd.merge(merged, rc_df, on='DT', how='left')\n",
    "    merged = pd.merge(merged, tw_df, on='DT', how='left')\n",
    "    \n",
    "    \n",
    "    # columns rename\n",
    "    merged.columns = ['datetime' , 'count', 'holiday', 'dayofweek',\n",
    "                      'long_h','b_long_h','inter_h','b_inter_h','first_m','b_first_m',\n",
    "                    'ticket_8', 'ticket_9', 'ticket_10','ticket_11','ticket_12','ticket_13','ticket_14',\n",
    "                    'ticket_15','ticket_16', 'ticket_17','ticket_18','ticket_19','ticket_20','ticket_21',\n",
    "                    'ticket_22', 'ticket_23', 'ticket_24','ticket_25','ticket_26','ticket_27','ticket_28',\n",
    "                    'fare_min_8','fare_min_9','fare_min_10','fare_min_11','fare_min_12','fare_min_13','fare_min_14',\n",
    "                    'fare_min_15','fare_min_16','fare_min_17','fare_min_18','fare_min_19','fare_min_20','fare_min_21',\n",
    "                    'fare_min_22','fare_min_23','fare_min_24','fare_min_25','fare_min_26','fare_min_27','fare_min_28',\n",
    "                    'fare_max_8','fare_max_9','fare_max_10','fare_max_11','fare_max_12','fare_max_13','fare_max_14',\n",
    "                    'fare_max_15','fare_max_16','fare_max_17','fare_max_18','fare_max_19','fare_max_20','fare_max_21',\n",
    "                    'fare_max_22','fare_max_23','fare_max_24','fare_max_25','fare_max_26','fare_max_27','fare_max_28',\n",
    "                    'fare_avg_8','fare_avg_9','fare_avg_10','fare_avg_11','fare_avg_12','fare_avg_13','fare_avg_14',\n",
    "                    'fare_avg_15','fare_avg_16','fare_avg_17','fare_avg_18','fare_avg_19','fare_avg_20','fare_avg_21',\n",
    "                    'fare_avg_22','fare_avg_23','fare_avg_24','fare_avg_25','fare_avg_26','fare_avg_27','fare_avg_28',\n",
    "                    'fare_std_8','fare_std_9','fare_std_10','fare_std_11','fare_std_12','fare_std_13','fare_std_14',\n",
    "                    'fare_std_15','fare_std_16','fare_std_17','fare_std_18','fare_std_19','fare_std_20','fare_std_21',\n",
    "                    'fare_std_22','fare_std_23','fare_std_24','fare_std_25','fare_std_26','fare_std_27','fare_std_28',\n",
    "                    'seat_min_8','seat_min_9','seat_min_10','seat_min_11','seat_min_12','seat_min_13','seat_min_14',\n",
    "                    'seat_min_15','seat_min_16','seat_min_17','seat_min_18','seat_min_19','seat_min_20','seat_min_21',\n",
    "                    'seat_min_22','seat_min_23','seat_min_24','seat_min_25','seat_min_26','seat_min_27','seat_min_28',\n",
    "                    'seat_max_8','seat_max_9','seat_max_10','seat_max_11','seat_max_12','seat_max_13','seat_max_14',\n",
    "                    'seat_max_15','seat_max_16','seat_max_17','seat_max_18','seat_max_19','seat_max_20','seat_max_21',\n",
    "                    'seat_max_22','seat_max_23','seat_max_24','seat_max_25','seat_max_26','seat_max_27','seat_max_28',\n",
    "                    'seat_avg_8','seat_avg_9','seat_avg_10','seat_avg_11','seat_avg_12','seat_avg_13','seat_avg_14',\n",
    "                    'seat_avg_15','seat_avg_16','seat_avg_17','seat_avg_18','seat_avg_19','seat_avg_20','seat_avg_21',\n",
    "                    'seat_avg_22','seat_avg_23','seat_avg_24','seat_avg_25','seat_avg_26','seat_avg_27','seat_avg_28',\n",
    "                    'seat_std_8','seat_std_9','seat_std_10','seat_std_11','seat_std_12','seat_std_13','seat_std_14',\n",
    "                    'seat_std_15','seat_std_16','seat_std_17','seat_std_18','seat_std_19','seat_std_20','seat_std_21',\n",
    "                    'seat_std_22','seat_std_23','seat_std_24','seat_std_25','seat_std_26','seat_std_27','seat_std_28',\n",
    "                    'b_8','b_9','b_10','b_11','b_12','b_13','b_14','b_15','b_16','b_17','b_18','b_19','b_20','b_21','b_22','b_23','b_24','b_25','b_26','b_27','b_28',\n",
    "                    'f_8','f_9','f_10','f_11','f_12','f_13','f_14','f_15','f_16','f_17','f_18','f_19','f_20','f_21','f_22','f_23','f_24','f_25','f_26','f_27','f_28',\n",
    "                    'd_8','d_9','d_10','d_11','d_12','d_13','d_14','d_15','d_16','d_17','d_18','d_19','d_20','d_21','d_22','d_23','d_24','d_25','d_26','d_27','d_28',\n",
    "                    's_8','s_9','s_10','s_11','s_12','s_13','s_14','s_15','s_16','s_17','s_18','s_19','s_20','s_21','s_22','s_23','s_24','s_25','s_26','s_27','s_28',\n",
    "                    'sd_8','sd_9','sd_10','sd_11','sd_12','sd_13','sd_14','sd_15','sd_16','sd_17','sd_18','sd_19','sd_20','sd_21','sd_22','sd_23','sd_24','sd_25','sd_26','sd_27','sd_28',\n",
    "                      'lt_8',   'lt_9',   'lt_10',  'lt_11',  'lt_12',  'lt_13', 'lt_14',\n",
    "                      'lt_15',   'lt_16',   'lt_17',  'lt_18',  'lt_19',  'lt_20', 'lt_21',\n",
    "                      'lt_22',   'lt_23',   'lt_24',   'lt_25',   'lt_26',   'lt_27' , 'lt_28',   \n",
    "                      'f330_8',   'f330_9',   'f330_10',  'f330_11',  'f330_12',  'f330_13', 'f330_14',\n",
    "                      'f330_15',  'f330_16',  'f330_17',  'f330_18',  'f330_19',  'f330_20', 'f330_21',\n",
    "                      'f330_22',  'f330_23',  'f330_24',  'f330_25',  'f330_26',  'f330_27' ,'f330_28', \n",
    "                      'f737_8',   'f737_9',   'f737_10',  'f737_11',  'f737_12',  'f737_13', 'f737_14',\n",
    "                      'f737_15',  'f737_16',  'f737_17',  'f737_18',  'f737_19',  'f737_20', 'f737_21',\n",
    "                      'f737_22',   'f737_23',   'f737_24',   'f737_25',   'f737_26',   'f737_27' , 'f737_28',   \n",
    "                      'group_8',   'group_9',   'group_10',  'group_11',  'group_12',  'group_13', 'group_14',\n",
    "                      'group_15',   'group_16',   'group_17',  'group_18',  'group_19',  'group_20', 'group_21',\n",
    "                      'group_22',   'group_23',   'group_24',   'group_25',   'group_26',   'group_27' , 'group_28', \n",
    "                      'total_8',   'total_9',   'total_10',  'total_11',  'total_12',  'total_13', 'total_14',\n",
    "                      'total_15',   'total_16',   'total_17',  'total_18',  'total_19',  'total_20', 'total_21',\n",
    "                      'total_22',   'total_23',   'total_24',   'total_25',   'total_26',   'total_27' , 'total_28', ]\n",
    "\n",
    "    \n",
    "    # for visitor count lag\n",
    "    merged_count=merged[['datetime','count']].sort_values(by='datetime')\n",
    "    merged = merged.loc[merged.datetime >= start_date,:].sort_values(by='datetime')\n",
    "    return merged, merged_count\n",
    "\n",
    "\n",
    "\n",
    "## Feature Engineering\n",
    "def feature_engineering(raw):\n",
    "    if 'datetime' in raw.columns:\n",
    "        raw['DateTime'] = pd.to_datetime(raw['datetime'])\n",
    "    if raw.index.dtype == 'int64':\n",
    "        raw.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    # add time series data \n",
    "    #raw['month'] = raw.datetime.dt.month\n",
    "    \n",
    "    # dummy 변수(카테고리 변수) => holiday(0,1) / dayofweek(0,1,2,3,4,5,6) / long_h(0,1) / b_long_h(0,1) / inter_h(0,1) / b_inter_h(0,1) \n",
    "    #                         /first_m(0,1)/ b_first_m(0,1) / year(2019,2020,2021,2022) / month(1~12) / quarter(1~4)\n",
    "\n",
    "    #raw = pd.concat([raw, pd.get_dummies(raw['holiday'],   prefix='holiday'   + '_dummy')], axis=1)\n",
    "    raw = pd.concat([raw, pd.get_dummies(raw['dayofweek'], prefix='dayofweek' + '_dummy')], axis=1)\n",
    "    #raw = pd.concat([raw, pd.get_dummies(raw['long_h'],    prefix='long_h'    + '_dummy')], axis=1)\n",
    "    #raw = pd.concat([raw, pd.get_dummies(raw['b_long_h'],  prefix='b_long_h'  + '_dummy')], axis=1)\n",
    "    #raw = pd.concat([raw, pd.get_dummies(raw['inter_h'],   prefix='inter_h'   + '_dummy')], axis=1)\n",
    "    #raw = pd.concat([raw, pd.get_dummies(raw['b_inter_h'], prefix='b_inter_h' + '_dummy')], axis=1)\n",
    "    #raw = pd.concat([raw, pd.get_dummies(raw['first_m'],   prefix='first_m'   + '_dummy')], axis=1)\n",
    "    #raw = pd.concat([raw, pd.get_dummies(raw['b_first_m'], prefix='b_first_m' + '_dummy')], axis=1)\n",
    "    #raw = pd.concat([raw, pd.get_dummies(raw['month'],     prefix='month'     + '_dummy')], axis=1)\n",
    "\n",
    "    raw = raw.drop(columns=['dayofweek'])\n",
    "    \n",
    "    raw_fe = raw.copy() \n",
    "    return raw_fe\n",
    "\n",
    "\n",
    "# Count lagged values of X_test\n",
    "def feature_engineering_lag_modified(Y, X, target):\n",
    "    X_lm = X.copy()\n",
    "    i = 0\n",
    "    for col in target:\n",
    "        X_lm[col] = Y.shift(i+14).values\n",
    "        X_lm[col].fillna(method='bfill', inplace=True)\n",
    "        i = i + 1\n",
    "    return X_lm\n",
    "\n",
    "# Data Split for Y & X\n",
    "def datasplit_X_Y(data, Y_colname, X_colname):\n",
    "    X_colname = [x for x in data.columns if x not in Y_colname + X_remove]\n",
    "    Y_data = data[Y_colname]\n",
    "    X_data = data[X_colname]\n",
    "    print('X:', X_data.shape, 'Y:', Y_data.shape)\n",
    "    return X_data, Y_data\n",
    "\n",
    "# Data Split for time series to train & test\n",
    "def datasplit_ts(raw, train_ratio):\n",
    "    size = int(len(raw) * train_ratio)\n",
    "    raw_train, raw_val = raw[0:size].copy(deep=True), raw[size:len(raw)].copy(deep=True)\n",
    "    print('Train_size:', raw_train.shape, 'Validation_size:', raw_val.shape)\n",
    "    return raw_train, raw_val\n",
    "\n",
    "# def datasplit_ts(raw, criteria):\n",
    "#     raw_train = raw.loc[merged.index < criteria, :]\n",
    "#     raw_validation  = raw.loc[merged.index >= criteria, :]\n",
    "#     print('Train_size:', raw_train.shape, 'Validation_size:', raw_validation.shape)\n",
    "#     return raw_train, raw_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d13c53-6a64-4b6f-93c1-0ea132f08ed6",
   "metadata": {},
   "source": [
    "#### 기간 설정 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0741ed1a-9dc2-45b1-a749-079f49662b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "yesterday = date.today() - timedelta(1)\n",
    "yesterday=yesterday.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32d78c9-18a7-47fa-9924-2bc188848418",
   "metadata": {},
   "outputs": [],
   "source": [
    "psg, air, wth, rc, tw = data_load(query_AP, query_AT, query_HD, query_RC, query_TW, '2022-01-01', yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d44ede-faa1-4d01-ba47-7fb81682a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "air = air.pivot(index='DT', columns='LEAD_TM', values=['TICKET', 'FARE_MIN', 'FARE_MAX', 'FARE_AVG', 'FARE_STD', 'SEAT_MIN', 'SEAT_MAX', 'SEAT_AVG', 'SEAT_STD', 'B', 'F', 'D', 'S', 'SD'])\n",
    "air.columns = air.columns.to_flat_index()\n",
    "air=air.reset_index()\n",
    "air2=air_dataprocessing(air)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c60f2f-c3d6-453a-a9a6-20a8c1f58130",
   "metadata": {
    "tags": []
   },
   "source": [
    "## function 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0064dbd-cd9a-40a6-bdef-88af6781b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Holiday Data Preprocessing\n",
    "wth_h = holiday_data_pre(wth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d53753-76d5-48cc-92c5-8c8e2bf4b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = rentacar_processing(rc)\n",
    "tw = tway_processing(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1874cd1-0354-478a-a1fa-7ba2df815725",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Merge\n",
    "merged, merged_count = data_merge(psg, wth_h, air2, rc, tw, '2022-02-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ebc9a90-3d2a-4e57-beec-145076d48b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (204, 421) Y: (204, 1)\n",
      "Train_size: (183, 421) Validation_size: (21, 421)\n",
      "Train_size: (183, 1) Validation_size: (21, 1)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "merged_fe  = feature_engineering(merged)\n",
    "\n",
    "# Modify count lagged values of X_train\n",
    "target_l = ['count_lag14','count_lag15', 'count_lag16','count_lag17', 'count_lag18','count_lag19','count_lag20']\n",
    "count_lag = feature_engineering_lag_modified(pd.DataFrame(merged_count['count']), pd.DataFrame(merged_count['datetime']), target_l)\n",
    "merged_fe=pd.merge(merged_fe, count_lag, on='datetime', how='left')\n",
    "\n",
    "# Data Split to X and Y\n",
    "Y_colname = ['count']\n",
    "X_remove = ['DateTime']\n",
    "X_colname = [x for x in merged_fe.columns if x not in Y_colname + X_remove]\n",
    "X, Y = datasplit_X_Y(merged_fe, Y_colname, X_colname)\n",
    "\n",
    "# Data Split for time series to train & test\n",
    "X_train, X_validation = datasplit_ts(X, 0.90)\n",
    "Y_train, Y_validation = datasplit_ts(Y, 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35adc6e6-6ee4-4de1-81ac-15a93f59d205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>40362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>42797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>43548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>39563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>39869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     count\n",
       "0    44620\n",
       "1    41036\n",
       "2    36365\n",
       "3    34202\n",
       "4    40057\n",
       "..     ...\n",
       "199  40362\n",
       "200  42797\n",
       "201  43548\n",
       "202  39563\n",
       "203  39869\n",
       "\n",
       "[204 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60aa632c-64bc-4946-9e10-98f6fbf00796",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds= pd.concat([X_train, Y_train], axis=1)\n",
    "valds = pd.concat([X_validation, Y_validation], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e1bc24b-3cc4-4299-9c3a-161027f3f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds['data_split']='TRAIN'\n",
    "valds['data_split'] ='VALIDATE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc76293c-cb04-4548-8dd2-a394e9f97831",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds.fillna(method = 'ffill',inplace=True) \n",
    "\n",
    "valds.fillna(method = 'ffill', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e20854e-56f1-4c88-8b9a-a6e209f072fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=charged-genre-350106, location=asia-northeast3, id=6ce40d9d-ec19-4071-a58e-56ab6cc4546f>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_id = \"charged-genre-350106.demand_forecasting.airport_passenger_tr_nm_ns_14_lt21_tr_fm\"\n",
    "\n",
    "job_config = bigquery.job.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "\n",
    "bqclient.load_table_from_dataframe(trainds, table_id, job_config=job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dd4be48-2fda-48db-bf93-1f6c44ce544f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=charged-genre-350106, location=asia-northeast3, id=406fadac-866c-48c2-876b-0050e08069ff>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_id = \"charged-genre-350106.demand_forecasting.airport_passenger_ts_nm_ns_14_lt21_tr_fm\"\n",
    "\n",
    "job_config = bigquery.job.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "\n",
    "bqclient.load_table_from_dataframe(valds, table_id, job_config=job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85c94e-688f-40f8-948a-3db5b0be8dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
